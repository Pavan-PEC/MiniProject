{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1131f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dhg\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, bias: bool = True, drop_rate: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.theta = nn.Linear(in_channels, out_channels, bias=bias)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, g: dhg.Graph) -> torch.Tensor:\n",
    "        if not isinstance(g, dhg.Graph):\n",
    "            raise TypeError(f\"Expected dhg.Graph, got {type(g)}\")  \n",
    "        X = self.theta(X)\n",
    "        X_ = g.smoothing_with_GCN(X)\n",
    "        X_ = self.drop(self.act(X_))\n",
    "        return X_\n",
    "\n",
    "class SSConv(nn.Module):\n",
    "    '''Spectral-Spatial Convolution'''\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3):\n",
    "        super(SSConv, self).__init__()\n",
    "        self.depth_conv = nn.Conv2d(\n",
    "            in_channels=out_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=kernel_size//2,\n",
    "            groups=out_ch\n",
    "        )\n",
    "        self.point_conv = nn.Conv2d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        )\n",
    "        self.Act1 = nn.LeakyReLU()\n",
    "        self.Act2 = nn.LeakyReLU()\n",
    "        self.BN = nn.BatchNorm2d(out_ch)  # Moved after point_conv\n",
    "        \n",
    "    def forward(self, input):\n",
    "        print(\"Input to point_conv:\", input.shape)\n",
    "        out = self.point_conv(input)\n",
    "        print(\"Output of point_conv:\", out.shape)  # Apply pointwise conv first\n",
    "        out = self.BN(out)  # Then batch normalization\n",
    "        out = self.Act1(out)\n",
    "        out = self.depth_conv(out)\n",
    "        out = self.Act2(out)\n",
    "        return out\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, edge_index: torch.Tensor, num_nodes: int):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.edge_index = edge_index\n",
    "        self.num_nodes = num_nodes\n",
    "        self.BN = nn.BatchNorm1d(input_dim)\n",
    "        self.Activation = nn.LeakyReLU()\n",
    "        \n",
    "        # GCN layers\n",
    "        self.GCN_liner_theta_1 = nn.Linear(input_dim, 256)\n",
    "        self.GCN_liner_out_1 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def normalize_adj(self):\n",
    "        \"\"\"Computes the normalized adjacency matrix.\"\"\"\n",
    "        adj = torch.zeros((self.num_nodes, self.num_nodes), device=self.edge_index.device)\n",
    "        adj[self.edge_index[0], self.edge_index[1]] = 1  # Convert edge list to adjacency matrix\n",
    "        D = adj.sum(1)\n",
    "        D_hat = torch.diag(torch.where(D > 0, torch.pow(D, -0.5), torch.zeros_like(D)))  # Avoid NaNs\n",
    "        return torch.mm(D_hat, torch.mm(adj, D_hat))\n",
    "\n",
    "    def forward(self, H):\n",
    "        H = self.BN(H)\n",
    "        A_hat = self.normalize_adj()\n",
    "        H_xx1 = self.GCN_liner_theta_1(H)\n",
    "        output = torch.mm(A_hat, self.GCN_liner_out_1(H_xx1))\n",
    "        return self.Activation(output)\n",
    "\n",
    "class CEGCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, class_count: int, edge_index: torch.Tensor, num_nodes: int):\n",
    "        super(CEGCN, self).__init__()\n",
    "        self.class_count = class_count\n",
    "        self.num_nodes = num_nodes\n",
    "        self.edge_index = edge_index\n",
    "        \n",
    "        # Spectral-Spatial Convolution\n",
    "        self.SSConv_layer = SSConv(input_dim, 128, kernel_size=3)\n",
    "\n",
    "        # GCN layers\n",
    "        self.GCN_Branch = nn.Sequential(\n",
    "            GCNLayer(128, 128, self.edge_index, self.num_nodes),\n",
    "            GCNLayer(128, 64, self.edge_index, self.num_nodes)\n",
    "        )\n",
    "\n",
    "        # Softmax classification layer\n",
    "        self.Softmax_linear = nn.Linear(64, self.class_count)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Forward pass using SSConv + GCN for superpixel classification.\"\"\"\n",
    "        x = x.unsqueeze(1)  # Shape [B, C, H, W]\n",
    "        print(\"Before SSConv:\", x.shape)  # Debugging\n",
    "        x = self.SSConv_layer(x)  # Apply spectral-spatial convolution\n",
    "        x = x.reshape(self.num_nodes, -1)  # Ensure correct shape\n",
    "        \n",
    "        H = self.GCN_Branch(x)  # Pass through GCN layers\n",
    "        \n",
    "        Y = self.Softmax_linear(H)  # Classification layer\n",
    "        return F.log_softmax(Y, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02736be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Pass edge_index instead of graph\n",
    "    out = model(data['x'])  # Forward pass\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(out[data['train_mask']], data['y'][data['train_mask']])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute accuracy\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[data['train_mask']].eq(data['y'][data['train_mask']]).sum().item()\n",
    "    acc = correct / data['train_mask'].sum().item()\n",
    "    \n",
    "    return loss.item(), acc\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        x = data['x'].to(device)  # Superpixel features\n",
    "        y_true = data['y'].cpu().numpy()  # True labels\n",
    "        output = model(x)\n",
    "        y_pred = output.argmax(dim=1).cpu().numpy()  # Predicted labels\n",
    "    \n",
    "    # Compute classification metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3862ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created superpixel graph with 91 nodes and 15936 edges.\n"
     ]
    }
   ],
   "source": [
    "%run data.ipynb\n",
    "\n",
    "PATH_TO_IMAGE = \"E:\\MINI_PROJECT\\PAVIA_UNI\\PaviaU.mat\"\n",
    "PATH_TO_LABEL = \"E:\\MINI_PROJECT\\PAVIA_UNI\\PaviaU_gt.mat\"\n",
    "\n",
    "data = DATA(PATH_TO_IMAGE, PATH_TO_LABEL, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc835c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643c9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e6f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_11812\\1998630865.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'x': torch.tensor(self.x, dtype=torch.float32),\n",
      "C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_11812\\1634272797.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_list = torch.stack([torch.tensor(graph_data['graph'].e_src, dtype=torch.long),\n",
      "C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_11812\\1634272797.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(graph_data['graph'].e_dst, dtype=torch.long)], dim=0)\n"
     ]
    }
   ],
   "source": [
    "graph_data = data.get_data_for_gcn()\n",
    "\n",
    "# Extract node features\n",
    "x = graph_data['x']\n",
    "\n",
    "# Extract edge list from graph\n",
    "edge_list = torch.stack([torch.tensor(graph_data['graph'].e_src, dtype=torch.long), \n",
    "                         torch.tensor(graph_data['graph'].e_dst, dtype=torch.long)], dim=0)\n",
    "\n",
    "num_nodes = x.shape[0]\n",
    "graph_data['edge_index'] = edge_list\n",
    "class_count = len(torch.unique(graph_data['y']))  # Number of classes\n",
    "\n",
    "# Initialize model\n",
    "model = CEGCN(input_dim=x.shape[1], class_count=class_count, edge_index=edge_list, num_nodes=num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f952f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SSConv: torch.Size([91, 1, 103])\n",
      "Input to point_conv: torch.Size([91, 1, 103])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 103, 1, 1], expected input[1, 91, 1, 103] to have 103 channels, but got 91 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 6\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train(model, graph_data, optimizer, criterion)\n\u001b[0;32m      7\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(model, graph_data, criterion)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, optimizer, criterion)\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Pass edge_index instead of graph\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m][data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 105\u001b[0m, in \u001b[0;36mCEGCN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape [B, C, H, W]\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore SSConv:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Debugging\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSSConv_layer(x)  \u001b[38;5;66;03m# Apply spectral-spatial convolution\u001b[39;00m\n\u001b[0;32m    106\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nodes, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Ensure correct shape\u001b[39;00m\n\u001b[0;32m    108\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGCN_Branch(x)  \u001b[38;5;66;03m# Pass through GCN layers\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mSSConv.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput to point_conv:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 47\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_conv(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput of point_conv:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Apply pointwise conv first\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBN(out)  \u001b[38;5;66;03m# Then batch normalization\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 103, 1, 1], expected input[1, 91, 1, 103] to have 103 channels, but got 91 channels instead"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, graph_data, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, graph_data, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf60af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd5b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
